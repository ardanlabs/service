apiVersion: apps/v1
kind: Deployment
metadata:
  name: sales
  namespace: sales-system

spec:
  replicas: 1

  strategy:
    type: Recreate

  template:
    spec:
      dnsPolicy: ClusterFirstWithHostNet
      hostNetwork: true

      # CPU Limits
      #
      # The request value helps K8s determine what node a POD can be run on by making
      # sure the number of requesting cores for the sum of all PODs are never greater
      # than the number of cores on a given node. If you have 8 PODs each requesting 1
      # core (1000m), then a node of 8 cores can run those 8 PODs. If you have 16 PODs
      # each requesting 1/2 core (500m), then a node of 8 cores can run those 16 PODs.
      #
      # The limit value helps the container runtime to determine which containers
      # (configured in a POD running on the node) can use a CPU and for how long. This
      # is measured in time where 100ms represents a unit of execution time. If a limit
      # of 1000m is requested, that means the container wants the full 100ms of time
      # all the time. There is no CPU affinity, but you can imagine the container is
      # given a full CPU to use on their own. A limit of 500m means the container wants
      # 50ms of the 100ms per cycle. You can imagine it gets to share a CPU half the
      # time with some other container.
      #
      # There are two points of view on these CPU quotas.
      #
      # Set the request to match the limit. The idea is if the container is requesting
      # 1/2 a CPU, then limit the execution time to 1/2 a CPU as well. This balances
      # the amount of CPU on the node with the amount of execution time. The drawback
      # is the container can't handle a burst of traffic because it might have used
      # it's allotted time and now it has to wait for the next 100ms cycle.
      #
      # Don't set any limit. The idea is allow all the containers to use the full
      # capacity of CPU's on the node it's running on. So if there are 8 CPUs on the
      # node, allow every program to use all 8 CPUs at the same time. This won't limit
      # a container from getting the CPU it needs, outside of competing with the other
      # containers.
      #
      # Determining the strategy depends on how you are measuring the performance of
      # the containers to determine when a new POD needs to be created on a different
      # node to handle the perceived load. Clusters have multiple dimensions of
      # autoscaling: Horizontal Pod Autoscaling, Vertical Pod Autoscaling, Autoscaling
      # of Nodes within a Node Group, Autoscaling of Node Groups themselves.

      # Nutty Swiss
      #
      # I’ve naively put jobs into roughly two buckets. The ones where access to CPU
      # cycles matter (CPU bound) and the ones where throttling is ok (IO Bound). For
      # CPU bound work, toss the Go program a decent number of cores, but not too much.
      # Allow for mobility of tasks within the cluster. For the IO bound work, toss
      # the Go program about a core, maybe less.
      #
      # In both cases, give the Go runtime a core and set your limit to (limit += 1)
      # since your Go program is CPU bound. Then let auto scaling take effect.
      #
      # Ideally you’d give tasks a reservation and no limit. However, I’ve never seen
      # anyone actually write software that deals with the situation where the job all
      # of a sudden does not get the “usual” over reservation cycles anymore. A
      # situation that usually happens during load spikes. Or outages, etc. Leave the
      # burst capable options to batch jobs.
      #
      # In the end, your tasks are a small player in a bigger game. Lifting the whole,
      # enabling the whole, is usually more important than optimizing just your workload.
      # Hence the 1/3 to 1/4 max core counts.

      # Nick Stogner
      #
      # Requests are separate from Limits b/c setting requests lower than limits allows
      # for more efficient resource utilization when you start packing multiple PODs
      # onto a given Node. Limits are there to prevent noisy neighbor conditions with
      # colocated PODs.

      # For CPU resource units, the quantity expression 0.1 is equivalent to the
      # expression 100m, which can be read as "one hundred millicpu".

      # Memory Limits
      #
      # This service will OOM when the K8s limit is at 17 MiB or below. However, I can
      # keep it alive at 13 MiB when setting GOMAXLIMIT to match. This is with the
      # GC off (GOGC=off).
      #
      # Memory Limits restrict the amount of memory the service can use. I have not
      # found any good rules to use, but here is some performance testing when
      # experimenting with memory limits using `make load`.
      #
      # Unrestricted:
      #   Total:        35.5745 secs
      #   Slowest:       1.1948 secs
      #   Fastest:       0.0048 secs
      #   Average:       0.3447 secs
      #   Requests/sec: 28.1100
      #
      # Limit: 18 MiB                  With GOMAXLIMIT: 18 MiB
      #   Total:        35.1985 secs     Total:        34.2020 secs
      #   Slowest:       1.1907 secs     Slowest:       1.1017 secs
      #   Fastest:       0.0054 secs     Fastest:       0.0042 secs
      #   Average:       0.3350 secs     Average:       0.3328 secs
      #   Requests/sec: 28.4103          Requests/sec: 29.2380
      #
      # Limit 23 MiB (Go Value)         With GOMAXLIMIT: 23 MiB
      #   Total:        33.5513 secs      Total:        29.9747 secs
      #   Slowest:       1.0979 secs      Slowest:       0.9976 secs
      #   Fastest:       0.0029 secs      Fastest:       0.0047 secs
      #   Average:       0.3285 secs      Average:       0.2891 secs
      #   Requests/sec: 29.8051           Requests/sec: 33.3615
      #
      # Limit 36 MiB                   With GOMAXLIMIT: 36 MiB
      #   Total:        35.3504 secs     Total:        28.2876 secs
      #   Slowest:       1.2809 secs     Slowest:       0.9867 secs
      #   Fastest:       0.0056 secs     Fastest:       0.0036 secs
      #   Average:       0.3393 secs     Average:       0.2763 secs
      #   Requests/sec: 28.2883          Requests/sec: 35.3512
      #
      # Limit 72 MiB                   With GOMAXLIMIT 72 MiB
      #   Total:        34.1320 secs     Total:        27.8793 secs
      #   Slowest:       1.2031 secs     Slowest:       0.9876 secs
      #   Fastest:       0.0033 secs     Fastest:       0.0046 secs
      #   Average:       0.3369 secs     Average:       0.2690 secs
      #   Requests/sec: 29.2980          Requests/sec: 35.8689

      containers:
        - name: sales
          resources:
            requests:
              cpu: "250m"
              memory: "100Mi"
            limits:
              cpu: "250m"
              memory: "100Mi"

        - name: metrics
          resources:
            requests:
              cpu: "250m"
            limits:
              cpu: "250m"
