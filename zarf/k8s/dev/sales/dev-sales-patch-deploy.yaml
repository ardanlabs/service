apiVersion: apps/v1
kind: Deployment
metadata:
  name: sales
  namespace: sales-system

spec:
  replicas: 1

  strategy:
    type: Recreate

  template:
    spec:
      dnsPolicy: ClusterFirstWithHostNet
      hostNetwork: true

# Compute Resource Quotas
#
# The request value helps K8s determine what node a POD can be run on by making
# sure the number of requesting cores for the sum of all PODs are never greater
# than the number of cores on a given node. If you have 8 PODs each requesting 1
# core (1000m), then a node of 8 cores can run those 8 PODs. If you have 16 PODs
# each requesting 1/2 core (500m), then a node of 8 cores can run those 16 PODs.
#
# The limit value helps the container runtime to determine which containers
# (configured in a POD running on the node) can use a CPU and for how long. This
# is measured in time where 100ms represents a unit of execution time. If a limit
# of 1000m is requested, that means the container wants the full 100ms of time
# all the time. There is no CPU affinity, but you can imagine the container is
# given a full CPU to use on their own. A limit of 500m means the container wants
# 50ms of the 100ms per cycle. You can imagine it gets to share a CPU half the
# time with some other container.
#
# There are two points of view on these CPU quotas.
#
# Set the request to match the limit. The idea is if the container is requesting
# 1/2 a CPU, then limit the execution time to 1/2 a CPU as well. This balances
# the amount of CPU on the node with the amount of execution time. The drawback
# is the container can't handle a burst of traffic because it might have used
# it's allotted time and now it has to wait for the next 100ms cycle.
#
# Don't set any limit. The idea is allow all the containers to use the full
# capacity of CPU's on the node it's running on. So if there are 8 CPUs on the
# node, allow every program to use all 8 CPUs at the same time. This won't limit
# a container from getting the CPU it needs, outside of competing with the other
# containers.
#
# Determining the strategy depends on how you are measuring the performance of
# the containers to determine when a new POD needs to be created on a different
# node to handle the perceived load. Clusters have multiple dimensions of
# autoscaling: Horizontal Pod Autoscaling, Vertical Pod Autoscaling, Autoscaling
# of Nodes within a Node Group, Autoscaling of Node Groups themselves.

# Nutty Swiss
#
# I’ve naively put jobs into roughly two buckets. The ones where access to CPU
# cycles matter (CPU bound) and the ones where throttling is ok (IO Bound). For
# CPU bound work, toss the Go program a decent number of cores, but not too much.
# Allow for mobility of tasks within the cluster. For the IO bound work, toss
# the Go program about a core, maybe less.
#
# In both cases, give the Go runtime a core and set your limit to (limit += 1)
# since your Go program is CPU bound. Then let auto scaling take effect.
#
# Ideally you’d give tasks a reservation and no limit. However, I’ve never seen
# anyone actually write software that deals with the situation where the job all
# of a sudden does not get the “usual” over reservation cycles anymore. A
# situation that usually happens during load spikes. Or outages, etc. Leave the
# burst capable options to batch jobs.
#
# In the end, your tasks are a small player in a bigger game. Lifting the whole,
# enabling the whole, is usually more important than optimizing just your workload.
# Hence the 1/3 to 1/4 max core counts.

# Nick Stogner
#
# Requests are separate from Limits b/c setting requests lower than limits allows
# for more efficient resource utilization when you start packing multiple PODs
# onto a given Node. Limits are there to prevent noisy neighbor conditions with
# colocated PODs.

# For CPU resource units, the quantity expression 0.1 is equivalent to the 
# expression 100m, which can be read as "one hundred millicpu". 

      containers:
      - name: sales-api
        resources:
          requests:
            cpu: "250m" # I need access to 1/4 of a core on the node.
          limits:
            cpu: "250m" # Execute instructions 25ms/100ms on my 1 core.

        volumeMounts:
          - name: vault
            mountPath: /vault

      - name: metrics
        resources:
          requests:
            cpu: "100m"
          limits:
            cpu: "100m"
